<!doctype html>







<html
  class="not-ready lg:text-base"
  style="--bg:#faf8f1"
  lang="zh-cn"
  dir="ltr"
><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title> - 自动驾驶/机器人学习笔记</title>

  
  <meta name="theme-color" />

  <meta name="description" content="每日自动驾驶/机器人模型学习计划
第一天：2026年1月13日
今日模型：RT-2（Robotics Transformer 2）

一、模型基本信息

  
      
          项目
          内容
      
  
  
      
          提出机构
          Google DeepMind (2023年)
      
      
          模型类型
          视觉-语言-动作多模态模型
      
      
          核心突破
          将机器人动作视为一种“语言”，实现互联网规模知识迁移到物理操作
      
      
          开源状态
          部分开源（代码与权重有限开放）
      
      
          学习优先级
          ⭐⭐⭐⭐⭐（机器人领域里程碑模型）
      
  


二、技术核心解析
1. 架构设计
RT-2 = Vision Transformer (ViT) &#43; 语言模型 (PaLM-E) &#43; 动作标记化
输入: [图像 &#43; 文本指令]
输出: [动作序列] 或 [语言回答 &#43; 动作]
2. 关键创新点

动作标记化：将机器人动作离散化为语言模型的词汇

例如：[GRASP, x=0.3, y=0.5, θ=0.1] → 词汇ID序列


网络规模知识迁移：利用预训练视觉-语言模型的常识推理能力
多任务统一：识别、推理、规划、执行统一到一个模型

3. 训练数据构成

机器人演示数据：13万条真实机器人轨迹
网络图像-文本对：数十亿级别（来自PaLM-E预训练）
合成数据：模拟器生成的指令-动作对


三、性能表现
基准测试结果

  
      
          任务类型
          RT-1
          RT-2
          提升
      
  
  
      
          未见物体操作
          32%
          62%
          &#43;30%
      
      
          符号理解任务
          0%
          40%
          &#43;40%
      
      
          多步推理任务
          20%
          45%
          &#43;25%
      
  

典型成功案例


抽象指令执行" />
  <meta name="author" content="自动驾驶/机器人学习笔记" /><link rel="preload stylesheet" as="style" href="https://loveppdog.github.io/main.min.569130d9047b717c8997ab0515954414dc24baf46708696a0a28a9ef212cb7e8.css" integrity="sha256-VpEw2QR7cXyJl6sFFZVEFNwkuvRnCGlqCiip7yEst&#43;g=" />

  
  <link rel="preload" as="image" href="https://loveppdog.github.io/theme.png" />

  

  

  <script
    defer
    src="https://loveppdog.github.io/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>

  
  <link
    rel="icon"
    href="https://loveppdog.github.io/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="https://loveppdog.github.io/apple-touch-icon.png"
  />

  <meta name="generator" content="Hugo 0.154.5">
  <meta itemprop="name" content="自动驾驶/机器人学习笔记">
  <meta itemprop="description" content="每日自动驾驶/机器人模型学习计划 第一天：2026年1月13日
今日模型：RT-2（Robotics Transformer 2）
一、模型基本信息 项目 内容 提出机构 Google DeepMind (2023年) 模型类型 视觉-语言-动作多模态模型 核心突破 将机器人动作视为一种“语言”，实现互联网规模知识迁移到物理操作 开源状态 部分开源（代码与权重有限开放） 学习优先级 ⭐⭐⭐⭐⭐（机器人领域里程碑模型） 二、技术核心解析 1. 架构设计 RT-2 = Vision Transformer (ViT) &#43; 语言模型 (PaLM-E) &#43; 动作标记化输入: [图像 &#43; 文本指令]输出: [动作序列] 或 [语言回答 &#43; 动作] 2. 关键创新点 动作标记化：将机器人动作离散化为语言模型的词汇 例如：[GRASP, x=0.3, y=0.5, θ=0.1] → 词汇ID序列 网络规模知识迁移：利用预训练视觉-语言模型的常识推理能力 多任务统一：识别、推理、规划、执行统一到一个模型 3. 训练数据构成 机器人演示数据：13万条真实机器人轨迹 网络图像-文本对：数十亿级别（来自PaLM-E预训练） 合成数据：模拟器生成的指令-动作对 三、性能表现 基准测试结果 任务类型 RT-1 RT-2 提升 未见物体操作 32% 62% &#43;30% 符号理解任务 0% 40% &#43;40% 多步推理任务 20% 45% &#43;25% 典型成功案例 抽象指令执行">
  <meta itemprop="wordCount" content="207"><meta property="og:url" content="https://loveppdog.github.io/posts/rt-2-20260123/">
  <meta property="og:site_name" content="自动驾驶/机器人学习笔记">
  <meta property="og:title" content="自动驾驶/机器人学习笔记">
  <meta property="og:description" content="每日自动驾驶/机器人模型学习计划 第一天：2026年1月13日
今日模型：RT-2（Robotics Transformer 2）
一、模型基本信息 项目 内容 提出机构 Google DeepMind (2023年) 模型类型 视觉-语言-动作多模态模型 核心突破 将机器人动作视为一种“语言”，实现互联网规模知识迁移到物理操作 开源状态 部分开源（代码与权重有限开放） 学习优先级 ⭐⭐⭐⭐⭐（机器人领域里程碑模型） 二、技术核心解析 1. 架构设计 RT-2 = Vision Transformer (ViT) &#43; 语言模型 (PaLM-E) &#43; 动作标记化输入: [图像 &#43; 文本指令]输出: [动作序列] 或 [语言回答 &#43; 动作] 2. 关键创新点 动作标记化：将机器人动作离散化为语言模型的词汇 例如：[GRASP, x=0.3, y=0.5, θ=0.1] → 词汇ID序列 网络规模知识迁移：利用预训练视觉-语言模型的常识推理能力 多任务统一：识别、推理、规划、执行统一到一个模型 3. 训练数据构成 机器人演示数据：13万条真实机器人轨迹 网络图像-文本对：数十亿级别（来自PaLM-E预训练） 合成数据：模拟器生成的指令-动作对 三、性能表现 基准测试结果 任务类型 RT-1 RT-2 提升 未见物体操作 32% 62% &#43;30% 符号理解任务 0% 40% &#43;40% 多步推理任务 20% 45% &#43;25% 典型成功案例 抽象指令执行">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="自动驾驶/机器人学习笔记">
  <meta name="twitter:description" content="每日自动驾驶/机器人模型学习计划 第一天：2026年1月13日
今日模型：RT-2（Robotics Transformer 2）
一、模型基本信息 项目 内容 提出机构 Google DeepMind (2023年) 模型类型 视觉-语言-动作多模态模型 核心突破 将机器人动作视为一种“语言”，实现互联网规模知识迁移到物理操作 开源状态 部分开源（代码与权重有限开放） 学习优先级 ⭐⭐⭐⭐⭐（机器人领域里程碑模型） 二、技术核心解析 1. 架构设计 RT-2 = Vision Transformer (ViT) &#43; 语言模型 (PaLM-E) &#43; 动作标记化输入: [图像 &#43; 文本指令]输出: [动作序列] 或 [语言回答 &#43; 动作] 2. 关键创新点 动作标记化：将机器人动作离散化为语言模型的词汇 例如：[GRASP, x=0.3, y=0.5, θ=0.1] → 词汇ID序列 网络规模知识迁移：利用预训练视觉-语言模型的常识推理能力 多任务统一：识别、推理、规划、执行统一到一个模型 3. 训练数据构成 机器人演示数据：13万条真实机器人轨迹 网络图像-文本对：数十亿级别（来自PaLM-E预训练） 合成数据：模拟器生成的指令-动作对 三、性能表现 基准测试结果 任务类型 RT-1 RT-2 提升 未见物体操作 32% 62% &#43;30% 符号理解任务 0% 40% &#43;40% 多步推理任务 20% 45% &#43;25% 典型成功案例 抽象指令执行">

  <link rel="canonical" href="https://loveppdog.github.io/posts/rt-2-20260123/" />
</head>
<body
    class="bg-(--bg) text-black antialiased duration-200 ease-out [-webkit-tap-highlight-color:transparent] dark:text-white"
  ><header
  class="mx-auto flex h-[4.5rem] max-w-(--w) px-8 whitespace-nowrap lg:justify-center"
>
  <div class="relative z-50 flex items-center ltr:mr-auto rtl:ml-auto">
    <a
      class="-translate-y-[1px] text-2xl font-medium"
      href="https://loveppdog.github.io/"
      >自动驾驶/机器人学习笔记</a
    >
    <div
      class="btn-dark text-[0px] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden ltr:-mr-8 rtl:-ml-8"
    role="button"
    aria-label="Menu"
  ></div>

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? "rgb(0 0 0 / 85%)" : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full flex-col justify-center bg-(--bg) pb-16 duration-200 select-none lg:static lg:h-auto lg:flex-row lg:bg-transparent! lg:pb-0 lg:transition-none"
  >
  </div>
</header>
<main
      class="prose prose-neutral dark:prose-invert relative mx-auto min-h-[calc(100vh-9rem)] max-w-(--w) px-8 pt-14 pb-16"
    ><article>
  <header class="mb-14">
    <h1 class="my-0! pb-2.5"></h1><div class="text-xs antialiased opacity-60"></div></header>

  <section><h1 id="每日自动驾驶机器人模型学习计划">每日自动驾驶/机器人模型学习计划</h1>
<p><strong>第一天：2026年1月13日</strong><br>
<strong>今日模型：RT-2（Robotics Transformer 2）</strong></p>
<hr>
<h2 id="一模型基本信息">一、模型基本信息</h2>
<table>
  <thead>
      <tr>
          <th>项目</th>
          <th>内容</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>提出机构</strong></td>
          <td>Google DeepMind (2023年)</td>
      </tr>
      <tr>
          <td><strong>模型类型</strong></td>
          <td>视觉-语言-动作多模态模型</td>
      </tr>
      <tr>
          <td><strong>核心突破</strong></td>
          <td>将机器人动作视为一种“语言”，实现互联网规模知识迁移到物理操作</td>
      </tr>
      <tr>
          <td><strong>开源状态</strong></td>
          <td>部分开源（代码与权重有限开放）</td>
      </tr>
      <tr>
          <td><strong>学习优先级</strong></td>
          <td>⭐⭐⭐⭐⭐（机器人领域里程碑模型）</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="二技术核心解析">二、技术核心解析</h2>
<h3 id="1-架构设计">1. <strong>架构设计</strong></h3>
<pre tabindex="0"><code>RT-2 = Vision Transformer (ViT) + 语言模型 (PaLM-E) + 动作标记化
输入: [图像 + 文本指令]
输出: [动作序列] 或 [语言回答 + 动作]
</code></pre><h3 id="2-关键创新点">2. <strong>关键创新点</strong></h3>
<ul>
<li><strong>动作标记化</strong>：将机器人动作离散化为语言模型的词汇
<ul>
<li>例如：<code>[GRASP, x=0.3, y=0.5, θ=0.1]</code> → 词汇ID序列</li>
</ul>
</li>
<li><strong>网络规模知识迁移</strong>：利用预训练视觉-语言模型的常识推理能力</li>
<li><strong>多任务统一</strong>：识别、推理、规划、执行统一到一个模型</li>
</ul>
<h3 id="3-训练数据构成">3. <strong>训练数据构成</strong></h3>
<ul>
<li>机器人演示数据：13万条真实机器人轨迹</li>
<li>网络图像-文本对：数十亿级别（来自PaLM-E预训练）</li>
<li>合成数据：模拟器生成的指令-动作对</li>
</ul>
<hr>
<h2 id="三性能表现">三、性能表现</h2>
<h3 id="基准测试结果">基准测试结果</h3>
<table>
  <thead>
      <tr>
          <th>任务类型</th>
          <th>RT-1</th>
          <th>RT-2</th>
          <th>提升</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>未见物体操作</td>
          <td>32%</td>
          <td>62%</td>
          <td>+30%</td>
      </tr>
      <tr>
          <td>符号理解任务</td>
          <td>0%</td>
          <td>40%</td>
          <td>+40%</td>
      </tr>
      <tr>
          <td>多步推理任务</td>
          <td>20%</td>
          <td>45%</td>
          <td>+25%</td>
      </tr>
  </tbody>
</table>
<h3 id="典型成功案例">典型成功案例</h3>
<ol>
<li>
<p><strong>抽象指令执行</strong></p>
<pre tabindex="0"><code>指令：“将可乐递给疲惫的人”
→ 模型识别可乐罐 → 识别“疲惫姿态”的人 → 执行递送动作
</code></pre></li>
<li>
<p><strong>符号推理</strong></p>
<pre tabindex="0"><code>指令：“移动到泰勒·斯威夫特曾经喝过的饮料处”
→ 识别饮料瓶 → 关联“泰勒·斯威夫特”与“可乐”文化符号
</code></pre></li>
</ol>
<hr>
<h2 id="四代码实践简化示例">四、代码实践（简化示例）</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># RT-2推理伪代码（基于公开资料还原）</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> rt2_model <span style="color:#f92672">import</span> RT2Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">RT2Inference</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, model_path):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> RT22Model<span style="color:#f92672">.</span>from_pretrained(model_path)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>action_tokenizer <span style="color:#f92672">=</span> ActionTokenizer()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">execute</span>(self, image, instruction):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 编码输入</span>
</span></span><span style="display:flex;"><span>        vision_features <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>encode_image(image)
</span></span><span style="display:flex;"><span>        text_features <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>encode_text(instruction)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 生成动作标记</span>
</span></span><span style="display:flex;"><span>        action_tokens <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>generate(
</span></span><span style="display:flex;"><span>            vision_input<span style="color:#f92672">=</span>vision_features,
</span></span><span style="display:flex;"><span>            text_input<span style="color:#f92672">=</span>text_features,
</span></span><span style="display:flex;"><span>            max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 解码为机器人动作</span>
</span></span><span style="display:flex;"><span>        actions <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>action_tokenizer<span style="color:#f92672">.</span>decode(action_tokens)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> actions
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用示例</span>
</span></span><span style="display:flex;"><span>robot <span style="color:#f92672">=</span> RT2Inference(<span style="color:#e6db74">&#34;google/rt-2-base&#34;</span>)
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#34;scene.jpg&#34;</span>)
</span></span><span style="display:flex;"><span>instruction <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;请将蓝色方块放在红色方块上面&#34;</span>
</span></span><span style="display:flex;"><span>action_sequence <span style="color:#f92672">=</span> robot<span style="color:#f92672">.</span>execute(image, instruction)
</span></span></code></pre></div><hr>
<h2 id="五局限性与挑战">五、局限性与挑战</h2>
<h3 id="技术局限">技术局限</h3>
<ol>
<li><strong>动作精度限制</strong>：离散化动作表示损失连续控制精度</li>
<li><strong>长序列规划弱</strong>：超过5步的任务成功率显著降低</li>
<li><strong>实时性不足</strong>：推理延迟约2-3秒，不适合高速动态环境</li>
</ol>
<h3 id="实际部署挑战">实际部署挑战</h3>
<ul>
<li>需要高精度相机和标定</li>
<li>对光照、遮挡敏感</li>
<li>缺乏安全保证机制</li>
</ul>
<hr>
<h2 id="六学习资源">六、学习资源</h2>
<h3 id="官方资源">官方资源</h3>
<ul>
<li><a href="https://robotics-transformer2.github.io/">https://robotics-transformer2.github.io/</a>（2023 Robotics: Science and Systems）</li>
<li><a href="https://deepmind.google/discover/blog/rt-2-new-model-controls-robots/">https://deepmind.google/discover/blog/rt-2-new-model-controls-robots/</a></li>
<li><a href="https://github.com/google-research/robotics_transformer">https://github.com/google-research/robotics_transformer</a></li>
</ul>
<h3 id="延伸学习">延伸学习</h3>
<ol>
<li><strong>前置知识</strong>：Vision Transformer, PaLM, 指令微调</li>
<li><strong>后续发展</strong>：RT-2-X（更大规模）、RT-H（人机协作）</li>
<li><strong>竞品对比</strong>：OpenAI的GPT-4V for Robotics, Tesla的Optimus系统</li>
</ol>
<hr>
<h2 id="七今日思考题">七、今日思考题</h2>
<ol>
<li>RT-2将动作离散化的优缺点是什么？连续控制能否用类似方法？</li>
<li>如何将RT-2的思路应用于自动驾驶（将驾驶动作视为“语言”）？</li>
<li>如果让你设计RT-3，会重点改进哪些方面？</li>
</ol>
<hr>
<h2 id="保存说明">保存说明</h2>
<p>本内容已自动保存为以下格式：</p>
<pre tabindex="0"><code>学习记录/
├── RT-2_学习笔记_20260113.md
├── RT-2_架构图.png（可在原论文中下载）
└── 代码示例/
    └── rt2_minimal_example.py
</code></pre><hr>
<p><strong>明天预告</strong>：VoxPoser——通过大语言模型生成机器人轨迹的模型</p>
<p>是否需要在当前内容中添加：<br>
① 更多代码实现细节<br>
② 与其他模型（如GATO, PALME）的对比表格<br>
③ 实际部署的硬件要求说明</p>
</section>

  <nav
    class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg leading-[1.2]! *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"
  ><a class="ltr:pr-3 rtl:pl-3" href="https://loveppdog.github.io/posts/2026-01-23/"
      ><span class="ltr:mr-1.5 rtl:ml-1.5">←</span><span>2026 01 23</span></a
    ><a
      class="justify-end pl-3 ltr:ml-auto rtl:mr-auto"
      href="https://loveppdog.github.io/posts/uniad-20260123/"
      ><span></span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a
    ></nav></article></main><footer
  class="mx-auto flex h-[4.5rem] max-w-(--w) items-center px-8 text-xs tracking-wider uppercase opacity-60"
>
  <div class="mr-auto">&copy;2026
    <a class="link" href="https://loveppdog.github.io/">自动驾驶/机器人学习笔记</a></div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>
</body>
</html>
